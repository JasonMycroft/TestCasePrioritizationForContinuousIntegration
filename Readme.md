# Test Case Prioritization Using Machine Learning in Continuous Integration

## Introduction
This repository is based on prior work, who's code can be found [here](https://github.com/Ahmadreza-SY/TCP-CI) and publication found [here](https://arxiv.org/abs/2109.13168).

## Environment Setup
### Python Environment
This project is tested on Python 3.7+. The required Python dependencies can be installed via the following command:
```bash
pip install -r requirements.txt
```

### Java
This project uses [RankLib](https://sourceforge.net/p/lemur/wiki/RankLib) for training and testing machine learning ranking models. RankLib is a library of learning-to-rank algorithms, and it is written in Java. Hence, this project required Java for running training and testing experiments. This project is trained and tested on JDK version `17.0.1`.

## Usage Instructions
First data must be collected from the [prior work](https://github.com/Ahmadreza-SY/TCP-CI). From that data we expect a new directory containing the `builds.csv` and `dataset.csv` (found in `\datasets\{projectName}\`) from each of the projects, where each filename is appended with an integer from 1-25 that identifies its project. For example we have a new directory `\datasets` with files `builds1.csv`, `dataset1.csv`, etc.

To compute the APFDc for each of the builds and projects, change the value of `datasets_dir` to that of your datasets and optionally change `results_dir` in `original_apfdc.py`. Now run `original_apfdc.py` and the APFDc results using the origianl test order are output to the `results_dir` folder. Alternatively you can set `shuffle` to `True` to compute the APFDc for a randomized order instead.

If you have generated results but for whatever reason an `apfdc_stats.csv` was not generated, it can be generated by changing the value of `results_dir` to the path of your results in `collect_stats.py` and running `collect_stats.py`.

If you have a directory of results (apfdc_stats.csv) and want to see if it is normally distributed, change `results_dir` in `check_normal.py` to that directory and run `check_normal.py`.

To run the experiment, run `main.py train`.\
Parameters:\
`-o`: Specifies the directory to save and load resulting datasets.\
`-t`: Number of recent builds to use for training of each model. [default 10]\
`-r`: Ranking algorithm used by RankLib ([0-8](https://sourceforge.net/p/lemur/wiki/RankLib%20How%20to%20use/)). [default 0]\
`-p`: Parameters passed to ranking algorithm. Parameters must use double dashes instead of single dashes. [dafault none]\
Example:\
python main.py train -o "G:\Downloads\!datasets" -r 8 -p "--tc -1 --mls 10"

To compare results of experiments, run `main.py results`.\
Parameters:\
`-o`: The output directory.\
`-a`: The path to the baseline results for the comparison. Expected file format is that of `apfdc_stats.csv`.\
`-b`: The path to the results for comparison. Expected file format is that of `apfdc_stats.csv`.\
Example:\
python main.py results -o "G:\Downloads\!datasets\!results\stat-analysis.csv" -a "G:\Downloads\!datasets\!results\baseline.csv" -b "G:\Downloads\!datasets\!results\MART.csv"
